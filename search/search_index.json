{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Accelerated WEKA - easy GPU support using WEKA Accelerated WEKA unifies the WEKA software , a well-known and open-source Java software, with new technologies that leverage the GPU to shorten the execution time of ML algorithms. It has two benefits aimed at users without expertise in system configuration and coding: an easy installation and a GUI that guides the configuration and execution of the ML tasks. Accelerated WEKA is a collection of packages available for WEKA (e.g., WDL4J , wekaPython , and wekaRAPIDS ). Accelerated WEKA can be easiy installed and anyone can extend it to support new tools and algorithms. Easy to install and use One of the best features of Accelerated WEKA is the installation method, which helps users without expertise in system configuration to bypass such issues as setting up different dependencies and environment variables. In addition, everything is accessible via the Weka GUI, the commandline and programmatically in Java. Check the Getting Started and the Examples sections.","title":"Home"},{"location":"#accelerated-weka-easy-gpu-support-using-weka","text":"Accelerated WEKA unifies the WEKA software , a well-known and open-source Java software, with new technologies that leverage the GPU to shorten the execution time of ML algorithms. It has two benefits aimed at users without expertise in system configuration and coding: an easy installation and a GUI that guides the configuration and execution of the ML tasks. Accelerated WEKA is a collection of packages available for WEKA (e.g., WDL4J , wekaPython , and wekaRAPIDS ). Accelerated WEKA can be easiy installed and anyone can extend it to support new tools and algorithms.","title":"Accelerated WEKA - easy GPU support using WEKA"},{"location":"#easy-to-install-and-use","text":"One of the best features of Accelerated WEKA is the installation method, which helps users without expertise in system configuration to bypass such issues as setting up different dependencies and environment variables. In addition, everything is accessible via the Weka GUI, the commandline and programmatically in Java. Check the Getting Started and the Examples sections.","title":"Easy to install and use"},{"location":"about/","text":"","title":"About"},{"location":"Introduction/architecture/","text":"The building blocks of Accelerated WEKA are packages like WekaDeeplearning4j and wekaRAPIDS (inspired by wekaPython). WekaDeeplearning4j (WDL4J) already supports GPU processing but has very specific needs in terms of libraries and environment configuration. WDL4J provides WEKA wrappers for the Deeplearning4j library. On the other hand, wekaPython originally provided Python integration by creating a server and communicating with it through sockets, enabling the user to execute scikit-learn ML algorithms (or even XGBoost) inside the WEKA workbench. Furthermore, wekaRAPIDS provide integration with RAPIDS cuML library by using the same technique in wekaPython. Together, both packages provide enhanced functionality and performance inside the user-friendly WEKA workbench. In fact, Accelerated WEKA goes a step further in the direction of performance by improving the communication between the JVM and Python interpreter. It does so by using alternatives like Apache Arrow and GPU memory sharing. This enables efficient data transfer between the two languages. Furthermore, Accelerated WEKA provides integration with the RAPIDS cuML library, which implements machine learning algorithms that are accelerated on NVIDIA GPUs. Some cuML algorithms can even support multi-GPU solutions. figure","title":"Architecture"},{"location":"Introduction/examples/","text":"","title":"Examples"},{"location":"Introduction/features/","text":"Features Single GPU support The algorithms currently supported by Accelerated WEKA are: - LinearRegression - LogisticRegression - Ridge - Lasso - ElasticNet - MBSGDClassifier - MBSGDRegressor - MultinomialNB - BernoulliNB - GaussianNB - RandomForestClassifier - RandomForestRegressor - SVC - SVR - LinearSVC - KNeighborsRegressor - KNeighborsClassifier Multi-GPU support The algorithms supported by Accelerated WEKA in multi-GPU mode are: - KNeighborsRegressor - KNeighborsClassifier - LinearRegression - Ridge - Lasso - ElasticNet - MultinomialNB - CD","title":"Features"},{"location":"Introduction/features/#features","text":"","title":"Features"},{"location":"Introduction/features/#single-gpu-support","text":"The algorithms currently supported by Accelerated WEKA are: - LinearRegression - LogisticRegression - Ridge - Lasso - ElasticNet - MBSGDClassifier - MBSGDRegressor - MultinomialNB - BernoulliNB - GaussianNB - RandomForestClassifier - RandomForestRegressor - SVC - SVR - LinearSVC - KNeighborsRegressor - KNeighborsClassifier","title":"Single GPU support"},{"location":"Introduction/features/#multi-gpu-support","text":"The algorithms supported by Accelerated WEKA in multi-GPU mode are: - KNeighborsRegressor - KNeighborsClassifier - LinearRegression - Ridge - Lasso - ElasticNet - MultinomialNB - CD","title":"Multi-GPU support"},{"location":"Introduction/getting_started/","text":"Running a quick example with the GUI Running a quick example with the command line To run a quick example with the command line there are two easy steps. First, let's create a small dataset just to get the hang of how to use Accelerated WEKA (except the new learner classes, it is the same as using standard WEKA): weka -main weka.Run .RandomRBF -n 10000 -a 50 > RBFa50n10k.arff Let's go through the arguments of the above command: weka is the main program, if you are using this command alone it launches the GUI. If you insert other arguments it can run tasks from the terminal. -main weka.Run indicates that we want to run the class weka.Run . In other words, we want to run straight from the command line, as opposed to the default weka.gui.GUIChooser that launches the GUI. .RandomRBF is the class that we want to use. This is a relative reference for the generator class that creates datasets with a Radial function. -n 10000 is one of the possible arguments for the RandomRBF class, it indicates that we want a dataset with ten thousand instances. -a 50 is another one of the RandomRBF arguments, it sets the number of attributes on the dataset to 50. >> RBFa50n10k.arff is the bash append operator followed by the file name that we want to write to. Then, let's use the newly created dataset to run some of the new RAPIDS algorithms using the GPU. weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn10k.arff -py-command python Again, let's go through the arguments of the above command: -memory 48g sets the JVM maximum heap to 48 gigabytes. weka.classifiers.rapids.CuMLClassifier is the class responsible for integrating RAPIDS to WEKA. -split-percentage 80 means that we want to split the dataset into two smaller ones. We should train with 80% of the dataset and test with the remaining 20%. -learner RandomForestClassifier indicates which RAPIDS classifier/regressor we want to use in our experiment. -t $(pwd)/RBFa5kn10k.arff sets the previously created dataset as the input for our experiment. -py-command python is an optional command just to make sure we are using the correct python command and to modify the python call in case we need to. After the code is run, you will get the result. Check accuracy and time taken. Now, let's run another RAPIDS learner with the same dataset. This time, try using the Support Vector classifier (SVC): weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner SVC -t $(pwd)/RBFa5kn10k.arff -py-command python Notice the only difference is the argument of the -learner option. Compare the results with the RandomForestClassifier. Feel free to explore the other supported learners from RAPIDS. You can find a comprehensive list of them in Features .","title":"Getting Started"},{"location":"Introduction/getting_started/#running-a-quick-example-with-the-gui","text":"","title":"Running a quick example with the GUI"},{"location":"Introduction/getting_started/#running-a-quick-example-with-the-command-line","text":"To run a quick example with the command line there are two easy steps. First, let's create a small dataset just to get the hang of how to use Accelerated WEKA (except the new learner classes, it is the same as using standard WEKA): weka -main weka.Run .RandomRBF -n 10000 -a 50 > RBFa50n10k.arff Let's go through the arguments of the above command: weka is the main program, if you are using this command alone it launches the GUI. If you insert other arguments it can run tasks from the terminal. -main weka.Run indicates that we want to run the class weka.Run . In other words, we want to run straight from the command line, as opposed to the default weka.gui.GUIChooser that launches the GUI. .RandomRBF is the class that we want to use. This is a relative reference for the generator class that creates datasets with a Radial function. -n 10000 is one of the possible arguments for the RandomRBF class, it indicates that we want a dataset with ten thousand instances. -a 50 is another one of the RandomRBF arguments, it sets the number of attributes on the dataset to 50. >> RBFa50n10k.arff is the bash append operator followed by the file name that we want to write to. Then, let's use the newly created dataset to run some of the new RAPIDS algorithms using the GPU. weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn10k.arff -py-command python Again, let's go through the arguments of the above command: -memory 48g sets the JVM maximum heap to 48 gigabytes. weka.classifiers.rapids.CuMLClassifier is the class responsible for integrating RAPIDS to WEKA. -split-percentage 80 means that we want to split the dataset into two smaller ones. We should train with 80% of the dataset and test with the remaining 20%. -learner RandomForestClassifier indicates which RAPIDS classifier/regressor we want to use in our experiment. -t $(pwd)/RBFa5kn10k.arff sets the previously created dataset as the input for our experiment. -py-command python is an optional command just to make sure we are using the correct python command and to modify the python call in case we need to. After the code is run, you will get the result. Check accuracy and time taken. Now, let's run another RAPIDS learner with the same dataset. This time, try using the Support Vector classifier (SVC): weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner SVC -t $(pwd)/RBFa5kn10k.arff -py-command python Notice the only difference is the argument of the -learner option. Compare the results with the RandomForestClassifier. Feel free to explore the other supported learners from RAPIDS. You can find a comprehensive list of them in Features .","title":"Running a quick example with the command line"},{"location":"Introduction/installation/","text":"Installation Accelerated WEKA was designed to provide an easy installation process. It can be done via the conda work with Python 3.8 and above. Installation can be done via pip : Accelerated WEKA simplifies the installation process by using the conda environment , making straightforward to use it from the beginning. Once you have conda installed, Accelerated WEKA can be installed by issuing the following command: conda create -n accelweka -c rapidsai -c nvidia -c conda-forge -c waikato weka Conda will take care of the configuration of dependencies, and after finishing the installation, you can start using Accelerated WEKA immediately by activating the newly created environment. conda activate accelweka And finally, launching WEKA GUI weka Alternatively, you can run stuff from the command line: weka -main weka.Run .RandomRBF -n 100000 -a 500 > RBFa500n100k.arff weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn1k.arff -py-command python Or Programatically through Java. Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"},{"location":"Introduction/installation/#installation","text":"Accelerated WEKA was designed to provide an easy installation process. It can be done via the conda work with Python 3.8 and above. Installation can be done via pip : Accelerated WEKA simplifies the installation process by using the conda environment , making straightforward to use it from the beginning. Once you have conda installed, Accelerated WEKA can be installed by issuing the following command: conda create -n accelweka -c rapidsai -c nvidia -c conda-forge -c waikato weka Conda will take care of the configuration of dependencies, and after finishing the installation, you can start using Accelerated WEKA immediately by activating the newly created environment. conda activate accelweka And finally, launching WEKA GUI weka Alternatively, you can run stuff from the command line: weka -main weka.Run .RandomRBF -n 100000 -a 500 > RBFa500n100k.arff weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn1k.arff -py-command python Or Programatically through Java. Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"}]}