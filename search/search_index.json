{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Accelerated WEKA - easy GPU support using WEKA Accelerated WEKA unifies the WEKA software , a well-known and open-source Java software, with new technologies that leverage the GPU to shorten the execution time of ML algorithms. It has two benefits aimed at users without expertise in system configuration and coding: an easy installation and a GUI that guides the configuration and execution of the ML tasks. Accelerated WEKA is a collection of packages available for WEKA (e.g., WDL4J , wekaPython , and wekaRAPIDS ). Accelerated WEKA can be easiy installed and anyone can extend it to support new tools and algorithms. Easy to install and use One of the best features of Accelerated WEKA is the installation method, which helps users without expertise in system configuration to bypass such issues as setting up different dependencies and environment variables. In addition, everything is accessible via the Weka GUI, the commandline and programmatically in Java. Check the Getting Started and the Examples sections.","title":"Home"},{"location":"#accelerated-weka-easy-gpu-support-using-weka","text":"Accelerated WEKA unifies the WEKA software , a well-known and open-source Java software, with new technologies that leverage the GPU to shorten the execution time of ML algorithms. It has two benefits aimed at users without expertise in system configuration and coding: an easy installation and a GUI that guides the configuration and execution of the ML tasks. Accelerated WEKA is a collection of packages available for WEKA (e.g., WDL4J , wekaPython , and wekaRAPIDS ). Accelerated WEKA can be easiy installed and anyone can extend it to support new tools and algorithms.","title":"Accelerated WEKA - easy GPU support using WEKA"},{"location":"#easy-to-install-and-use","text":"One of the best features of Accelerated WEKA is the installation method, which helps users without expertise in system configuration to bypass such issues as setting up different dependencies and environment variables. In addition, everything is accessible via the Weka GUI, the commandline and programmatically in Java. Check the Getting Started and the Examples sections.","title":"Easy to install and use"},{"location":"about/","text":"Package links WEKA wekaPython wekaDeeplearning4j wekaRAPIDS","title":"About"},{"location":"about/#package-links","text":"WEKA wekaPython wekaDeeplearning4j wekaRAPIDS","title":"Package links"},{"location":"user_guide/architecture/","text":"The building blocks of Accelerated WEKA are packages like WekaDeeplearning4j and wekaRAPIDS (inspired by wekaPython ). WekaDeeplearning4j (WDL4J) already supports GPU processing but has very specific needs in terms of libraries and environment configuration. WDL4J provides WEKA wrappers for the Deeplearning4j library. On the other hand, wekaPython originally provided Python integration by creating a server and communicating with it through sockets, enabling the user to execute scikit-learn ML algorithms (or even XGBoost) inside the WEKA workbench. Furthermore, wekaRAPIDS provide integration with RAPIDS cuML library using a similar technique to wekaPython. Together, both packages provide enhanced functionality and performance inside the user-friendly WEKA workbench. In fact, Accelerated WEKA goes a step further in the direction of performance by improving the communication between the JVM and Python interpreter. It does so by using alternatives like Apache Arrow and GPU memory sharing. This enables efficient data transfer between the two languages. Furthermore, Accelerated WEKA provides integration with the RAPIDS cuML library, which implements machine learning algorithms that are accelerated on NVIDIA GPUs. Some cuML algorithms can even support multi-GPU solutions.","title":"Architecture"},{"location":"user_guide/examples/","text":"","title":"Examples"},{"location":"user_guide/features/","text":"Accelerated WEKA provides support to all algorithms from the previous packages. In addition, wekaRAPIDS provides integration with the RAPIDS library, which offers both Single and Multi-GPU support. Below, you can find the RAPIDS algorithms currently supported by wekaRAPIDS. Single GPU support The algorithms currently supported by Accelerated WEKA are: LinearRegression LogisticRegression Ridge Lasso ElasticNet MBSGDClassifier MBSGDRegressor MultinomialNB BernoulliNB GaussianNB RandomForestClassifier RandomForestRegressor SVC SVR LinearSVC KNeighborsRegressor KNeighborsClassifier Multi-GPU support The algorithms supported by Accelerated WEKA in multi-GPU mode are: KNeighborsRegressor KNeighborsClassifier LinearRegression Ridge Lasso ElasticNet MultinomialNB CD","title":"Features"},{"location":"user_guide/features/#single-gpu-support","text":"The algorithms currently supported by Accelerated WEKA are: LinearRegression LogisticRegression Ridge Lasso ElasticNet MBSGDClassifier MBSGDRegressor MultinomialNB BernoulliNB GaussianNB RandomForestClassifier RandomForestRegressor SVC SVR LinearSVC KNeighborsRegressor KNeighborsClassifier","title":"Single GPU support"},{"location":"user_guide/features/#multi-gpu-support","text":"The algorithms supported by Accelerated WEKA in multi-GPU mode are: KNeighborsRegressor KNeighborsClassifier LinearRegression Ridge Lasso ElasticNet MultinomialNB CD","title":"Multi-GPU support"},{"location":"user_guide/getting_started/","text":"AcceleratedWEKA offers the same options as the standard WEKA. Running a quick example with the GUI The installation of Accelerated WEKA itself is pretty simple. It is available through Conda, a system providing package and environment management. Such capability means that a simple command can install all dependencies for the project. For example, on a Linux machine, issue the following command in a terminal for installing Accelerated WEKA and all dependencies. conda create -n accelweka -c rapidsai -c nvidia -c conda-forge -c waikato weka Once Conda has created the environment, activate it with the following command: conda activate accelweka This terminal instance just loaded all dependencies for Accelerated WEKA. Launch WEKA GUI Chooser with the command: weka From there, click the Explorer button to access the functionalities of Accelerated WEKA. In the WEKA Explorer window, click the Open file button to select a dataset file. WEKA works with ARFF files but can read from CSVs. Converting from CSVs can be pretty straightforward or require some configuration by the user, depending on the types of the attributes. Assuming one does not want to preprocess the data, clicking the Classify tab will present the classification options to the user. In this tab, the user can configure the classification algorithm and the test options that are going to be used in the experiment using the previously selected dataset. Clicking \u201cChoose\u201d button will show the implemented classifiers. Some might be disabled because of the dataset characteristics. To use Accelerated WEKA, the user must select rapids.CuMLClassifier. After that, clicking the bold CuMLClassifier will take the user to the option windows for the classifier. After configuring the Classifier according to the previous step, the parameters will be shown in the text field beside the Choose button. After clicking Start, WEKA will start executing the chosen classifier with the dataset. The following figure shows the classifier in action, the Classifier output is showing debug and general information regarding the experiment, such as parameters, classifiers, dataset, and test options. The status shows the current state of the execution and the Weka bird on the bottom animates and flips from one side to the other while the experiment is running. After the algorithm finishes the task it will output the summary of the execution with information regarding predictive performance and the time taken. In the following figure, the output shows the results for 10-fold cross-validation using the RandomForestClassifier from cuML through CuMLClassifier. Running a quick example with the command line To run a quick example with the command line there are two easy steps. First, let's create a small dataset just to get the hang of how to use Accelerated WEKA (except the new learner classes, it is the same as using standard WEKA): weka -main weka.Run .RandomRBF -n 10000 -a 50 > RBFa50n10k.arff Let's go through the arguments of the above command: weka is the main program, if you are using this command alone it launches the GUI. If you insert other arguments it can run tasks from the terminal. -main weka.Run indicates that we want to run the class weka.Run . In other words, we want to run straight from the command line, as opposed to the default weka.gui.GUIChooser that launches the GUI. .RandomRBF is the class that we want to use. This is a relative reference for the generator class that creates datasets with a Radial function. -n 10000 is one of the possible arguments for the RandomRBF class, it indicates that we want a dataset with ten thousand instances. -a 50 is another one of the RandomRBF arguments, it sets the number of attributes on the dataset to 50. >> RBFa50n10k.arff is the bash append operator followed by the file name that we want to write to. Then, let's use the newly created dataset to run some of the new RAPIDS algorithms using the GPU. weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn10k.arff -py-command python Again, let's go through the arguments of the above command: -memory 48g sets the JVM maximum heap to 48 gigabytes. weka.classifiers.rapids.CuMLClassifier is the class responsible for integrating RAPIDS to WEKA. -split-percentage 80 means that we want to split the dataset into two smaller ones. We should train with 80% of the dataset and test with the remaining 20%. -learner RandomForestClassifier indicates which RAPIDS classifier/regressor we want to use in our experiment. -t $(pwd)/RBFa5kn10k.arff sets the previously created dataset as the input for our experiment. -py-command python is an optional command just to make sure we are using the correct python command and to modify the python call in case we need to. After the code is run, you will get the result. Check accuracy and time taken. Now, let's run another RAPIDS learner with the same dataset. This time, try using the Support Vector classifier (SVC): weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner SVC -t $(pwd)/RBFa5kn10k.arff -py-command python Notice the only difference is the argument of the -learner option. Compare the results with the RandomForestClassifier. Feel free to explore the other supported learners from RAPIDS. You can find a comprehensive list of them in Features .","title":"Getting Started"},{"location":"user_guide/getting_started/#running-a-quick-example-with-the-gui","text":"The installation of Accelerated WEKA itself is pretty simple. It is available through Conda, a system providing package and environment management. Such capability means that a simple command can install all dependencies for the project. For example, on a Linux machine, issue the following command in a terminal for installing Accelerated WEKA and all dependencies. conda create -n accelweka -c rapidsai -c nvidia -c conda-forge -c waikato weka Once Conda has created the environment, activate it with the following command: conda activate accelweka This terminal instance just loaded all dependencies for Accelerated WEKA. Launch WEKA GUI Chooser with the command: weka From there, click the Explorer button to access the functionalities of Accelerated WEKA. In the WEKA Explorer window, click the Open file button to select a dataset file. WEKA works with ARFF files but can read from CSVs. Converting from CSVs can be pretty straightforward or require some configuration by the user, depending on the types of the attributes. Assuming one does not want to preprocess the data, clicking the Classify tab will present the classification options to the user. In this tab, the user can configure the classification algorithm and the test options that are going to be used in the experiment using the previously selected dataset. Clicking \u201cChoose\u201d button will show the implemented classifiers. Some might be disabled because of the dataset characteristics. To use Accelerated WEKA, the user must select rapids.CuMLClassifier. After that, clicking the bold CuMLClassifier will take the user to the option windows for the classifier. After configuring the Classifier according to the previous step, the parameters will be shown in the text field beside the Choose button. After clicking Start, WEKA will start executing the chosen classifier with the dataset. The following figure shows the classifier in action, the Classifier output is showing debug and general information regarding the experiment, such as parameters, classifiers, dataset, and test options. The status shows the current state of the execution and the Weka bird on the bottom animates and flips from one side to the other while the experiment is running. After the algorithm finishes the task it will output the summary of the execution with information regarding predictive performance and the time taken. In the following figure, the output shows the results for 10-fold cross-validation using the RandomForestClassifier from cuML through CuMLClassifier.","title":"Running a quick example with the GUI"},{"location":"user_guide/getting_started/#running-a-quick-example-with-the-command-line","text":"To run a quick example with the command line there are two easy steps. First, let's create a small dataset just to get the hang of how to use Accelerated WEKA (except the new learner classes, it is the same as using standard WEKA): weka -main weka.Run .RandomRBF -n 10000 -a 50 > RBFa50n10k.arff Let's go through the arguments of the above command: weka is the main program, if you are using this command alone it launches the GUI. If you insert other arguments it can run tasks from the terminal. -main weka.Run indicates that we want to run the class weka.Run . In other words, we want to run straight from the command line, as opposed to the default weka.gui.GUIChooser that launches the GUI. .RandomRBF is the class that we want to use. This is a relative reference for the generator class that creates datasets with a Radial function. -n 10000 is one of the possible arguments for the RandomRBF class, it indicates that we want a dataset with ten thousand instances. -a 50 is another one of the RandomRBF arguments, it sets the number of attributes on the dataset to 50. >> RBFa50n10k.arff is the bash append operator followed by the file name that we want to write to. Then, let's use the newly created dataset to run some of the new RAPIDS algorithms using the GPU. weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn10k.arff -py-command python Again, let's go through the arguments of the above command: -memory 48g sets the JVM maximum heap to 48 gigabytes. weka.classifiers.rapids.CuMLClassifier is the class responsible for integrating RAPIDS to WEKA. -split-percentage 80 means that we want to split the dataset into two smaller ones. We should train with 80% of the dataset and test with the remaining 20%. -learner RandomForestClassifier indicates which RAPIDS classifier/regressor we want to use in our experiment. -t $(pwd)/RBFa5kn10k.arff sets the previously created dataset as the input for our experiment. -py-command python is an optional command just to make sure we are using the correct python command and to modify the python call in case we need to. After the code is run, you will get the result. Check accuracy and time taken. Now, let's run another RAPIDS learner with the same dataset. This time, try using the Support Vector classifier (SVC): weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner SVC -t $(pwd)/RBFa5kn10k.arff -py-command python Notice the only difference is the argument of the -learner option. Compare the results with the RandomForestClassifier. Feel free to explore the other supported learners from RAPIDS. You can find a comprehensive list of them in Features .","title":"Running a quick example with the command line"},{"location":"user_guide/installation/","text":"Accelerated WEKA was designed to provide an easy installation process. Accelerated WEKA simplifies the installation process by using the conda environment . This makes straightforward to use Accelerated WEKA from the beginning. Once you have conda installed, Accelerated WEKA can be installed by issuing the following command: conda create -n accelweka -c rapidsai -c nvidia -c conda-forge -c waikato weka Conda will take care of the configuration of dependencies, and after finishing the installation, you can start using Accelerated WEKA immediately by activating the newly created environment. conda activate accelweka And finally, launching WEKA GUI weka Alternatively, you can run stuff from the command line: weka -main weka.Run .RandomRBF -n 100000 -a 500 > RBFa500n100k.arff weka -memory 48g -main weka.Run weka.classifiers.rapids.CuMLClassifier -split-percentage 80 -learner RandomForestClassifier -t $(pwd)/RBFa5kn1k.arff -py-command python Or Programatically through Java. Feel welcome to open an issue on GitHub if you are having any trouble.","title":"Installation"}]}